{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PyDOE: An Experimental Design Package for Python","text":"<p>The <code>pyDOE</code> package is designed to help the scientist, engineer, statistician, etc., to construct appropriate  experimental designs.</p> <p>Quick Start</p> <p>All available designs can be accessed after a simple import statement:</p> <pre><code>&gt;&gt;&gt; from pyDOE import *\n</code></pre>"},{"location":"contributing/","title":"Contributing to PyDOE","text":"<p>Thank you for your interest in contributing to PyDOE!  This guide explains how to report issues, propose changes, and maintain consistency in code, documentation, and testing.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":"<p>You can contribute in several ways:</p> <ul> <li>Reporting bugs or suggesting improvements via GitHub Issues.</li> <li>Answering questions and helping other users.</li> <li>Submitting pull requests.</li> <li>Improving or expanding documentation.</li> <li>Adding or improving tests.</li> <li>Reviewing open pull requests. It is educational and productive.</li> </ul> <p>Before starting significant work (other than fixing small typos in documentation or small bug fixes), please open an issue to discuss your proposal.</p>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>When filing an issue:</p> <ul> <li>Search existing issues to avoid duplicates.</li> <li>Use a clear and descriptive title starting with relevant keywords like <code>feature-req -</code>, <code>bug -</code> wherever possible.</li> <li>Provide detailed steps to reproduce the problem (for bug reports only).</li> <li>Describe the expected and actual behavior.</li> <li>Include environment details (Python version, OS, dependencies etc.).</li> <li>If possible, provide a minimal reproducible example.</li> </ul>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<p>You will need following prerequisites for getting started with development:</p> <ul> <li>git</li> <li>uv</li> <li>any code editor/IDE of your choice like vscode, pycharm etc.</li> </ul>"},{"location":"contributing/#installation-and-setup","title":"Installation and Setup","text":"<p>Fork the repo on GitHub and clone it to your local machine.</p> <pre><code>git clone https://github.com/&lt;your-username&gt;/pydoe.git --branch main --depth 1\ncd pydoe\nuv sync\n</code></pre>"},{"location":"contributing/#commit-changes","title":"Commit Changes","text":"<p>Switch to a new branch for your changes.</p> <pre><code>git checkout -b &lt;your-branch-name&gt;\n</code></pre>"},{"location":"contributing/#run-linter-and-tests","title":"Run Linter and Tests","text":"<p>Add and updata tests to ensure that tests cover your changes. Run tests and linter to ensure everything is consistent and working correctly.</p> <pre><code># running linter and formatter\nuvx ruff format .\nuvx ruff check --fix .\n\n# running tests\nuv run pytest -n auto tests\n</code></pre>"},{"location":"contributing/#build-documentation","title":"Build Documentation","text":"<p>It is recommeded to add documentation about your change using docstrings or editing docs files. If you have made any changes to the documentation, build and preview it locally. API docs are auto-generated from docstrings, however you need to add function manually to the relevant docs file. Be sure to check the warnings and errors while building the docs.</p> <pre><code>uv run mkdocs serve --livereload\n</code></pre>"},{"location":"contributing/#open-a-pull-request","title":"Open a Pull Request","text":"<p>After everything is finalized, commit your changes with a descriptive commit message, push your changes to your fork, and open a DRAFT pull request against the <code>main</code> branch of the parent repo. Be sure to provide a good description and mention any related issues.</p> <p>Once your changes are ready for review, you can mark the PR as ready for review and some maintainer will review it.</p>"},{"location":"credits/","title":"Credits","text":"<p>The evolution of PyDOE reflects contributions from multiple individuals and communities over time.</p>"},{"location":"credits/#scilab-scidoe","title":"Scilab Scidoe","text":"<p>The code that eventually became PyDOE was originally developed for Scilab by:</p> <ul> <li>Copyright (c) 2012\u20132013, Michael Baudin</li> <li>Copyright (c) 2012, Maria Christopoulou</li> <li>Copyright (c) 2010\u20132011, INRIA, Michael Baudin</li> <li>Copyright (c) 2009, Yann Collette</li> <li>Copyright (c) 2009, CEA, Jean-Marc Martinez</li> </ul> <p>Sources: gitlab.com/scilab/forge/scidoe atoms.scilab.org/toolboxes/scidoe</p>"},{"location":"credits/#pydoe","title":"PyDOE","text":"<p>The original Scilab code was converted to Python and packaged as PyDOE by:</p> <ul> <li>Copyright (c) 2014, Abraham D. Lee</li> </ul> <p>This version provided Python users with factorial, fractional factorial, Plackett-Burman, Box-Behnken, central composite, and Latin-hypercube designs.</p> <p>Source: github.com/tisimst/pydoe</p>"},{"location":"credits/#pydoe2","title":"PyDOE2","text":"<p>PyDOE2 is a fork of PyDOE, created by:</p> <ul> <li>Copyright (c) 2018, Rickard Sj\u00f6gren and Daniel Svensson</li> </ul> <p>PyDOE2 added features such as Generalized Subset Designs (GSD) and modernized the codebase.</p> <p>Source: github.com/clicumu/pydoe2</p>"},{"location":"credits/#pydoe3","title":"PyDOE3","text":"<p>PyDOE3 is a fork of PyDOE2, created by:</p> <ul> <li>Copyright (c) 2023, R\u00e9mi Lafage</li> </ul> <p>PyDOE3 added several new design families (optimal, sparse, Taguchi, sensitivity), fixed long-standing bugs, and continued active development.</p> <p>Source: github.com/relf/pydoe3</p> <p>All previous versions and forks, including PyDOE2 and PyDOE3, have now been consolidated under a single active repository: github.com/pydoe/pydoe</p>"},{"location":"installation/","title":"Installation","text":"<p><code>pyDOE</code> can be installed from <code>pypi</code>, <code>conda-forge</code>, and <code>git</code>.</p>"},{"location":"installation/#pypi","title":"PyPI","text":"<p>For using the PyPI package in your project, you can update your configuration file by adding following snippet.</p> pyproject.tomlrequirements.txt <pre><code>[project.dependencies]\npyDOE = \"*\" # (1)!\n</code></pre> <ol> <li>Specifying a version is recommended</li> </ol> <pre><code>pyDOE&gt;=0.3.8\n</code></pre>"},{"location":"installation/#pip","title":"pip","text":"Installation for userInstallation in virtual environment <pre><code>pip install --upgrade --user pyDOE # (1)!\n</code></pre> <ol> <li>You may need to use <code>pip3</code> instead of <code>pip</code> depending on your python installation.</li> </ol> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install --require-virtualenv --upgrade pyDOE # (1)!\n</code></pre> <ol> <li>You may need to use <code>pip3</code> instead of <code>pip</code> depending on your python installation.</li> </ol> <p>Note</p> <p>Command to activate the virtual env depends on your platform and shell. More info</p>"},{"location":"installation/#pipenv","title":"pipenv","text":"<pre><code>pipenv install pyDOE\n</code></pre>"},{"location":"installation/#uv","title":"uv","text":"Adding to uv projectInstalling to uv environment <pre><code>uv add pyDOE\nuv sync\n</code></pre> <pre><code>uv venv\nuv pip install pyDOE\n</code></pre>"},{"location":"installation/#poetry","title":"poetry","text":"<pre><code>poetry add pyDOE\n</code></pre>"},{"location":"installation/#pdm","title":"pdm","text":"<pre><code>pdm add pyDOE\n</code></pre>"},{"location":"installation/#hatch","title":"hatch","text":"<pre><code>hatch add pyDOE\n</code></pre>"},{"location":"installation/#conda-forge","title":"conda-forge","text":"<p>You can update your environment spec file by adding following snippets.</p> environment.yml<pre><code>channels:\n  - conda-forge\ndependencies:\n  - pyDOE # (1)!\n</code></pre> <ol> <li>Specifying a version is recommended</li> </ol> <p>Installation can be done using the updated environment spec file.</p> condamicromamba <pre><code>conda env update --file environment.yml\n</code></pre> <pre><code>micromamba env update --file environment.yml\n</code></pre> <p>Note</p> <p>replace <code>environment.yml</code> with your actual environment spec file name if it's different.</p> <p>Or directly in your conda environment.</p> condamicromamba <pre><code>conda install -c conda-forge pyDOE\n</code></pre> <pre><code>micromamba install -c conda-forge pyDOE\n</code></pre>"},{"location":"installation/#git","title":"git","text":"<p>Sometimmes release can fall behind the latest changes in the repository. <code>pip</code> can directly install from the git repository.</p> <pre><code>pip install --upgrade \"git+github.com/pydoe/pyDOE.git#egg=pyDOE\"\n</code></pre>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<ul> <li>Python Installation see supported python versions</li> <li>numpy</li> <li>scipy</li> </ul>"},{"location":"license/","title":"License","text":"<p>Copyright (c) 2014, Abraham D. Lee</p> <p>Copyright (c) 2018, Rickard Sj\u00f6gren &amp; Daniel Svensson</p> <p>Copyright (c) 2023, R\u00e9mi Lafage</p> <p>All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"users/","title":"Who uses PyDOE?","text":"<p>PyDOE is a widely used Python library for design of experiments (DOE). Its users range from individual researchers to global organizations, spanning academia, industry, and open-source projects. It supports applications in simulation-based optimization, engineering workflows, scientific research, and data-driven model development.</p> <p>Selected Testimonials Below are some small selection of the many institutes and companies that use PyDOE.</p> <p> GlennOPT is a Python optimization tool developed by NASA for multi-objective optimization of engineering simulations (e.g., CFD problems). It uses evolutionary strategies (like Differential Evolution and NSGA methods) to explore design spaces, handle simulation failures, and restart long-running jobs robustly. It\u2019s designed to track evaluations and integrate with external simulation workflows. It uses PyDOE to generate design-of-experiments samples for multi-objective simulation-based optimization.</p> <p>Source: github.com/nasa/GlennOPT</p> <p></p> <p> IBM's SimulAI is a Python toolkit for physics-informed machine learning, combining models like Physics-Informed Neural Networks (PINNs), DeepONets, autoencoders, etc., for scientific computing and reduced-order modeling. It\u2019s intended to unify state-of-the-art ML techniques for physical systems.</p> <p>Source: github.com/ibm/simulai</p> <p></p> <p> Holodeck is an astrophysics software package developed by NANOGrav for massive black-hole binary population synthesis and for generating synthetic populations of supermassive black hole binaries for gravitational-wave background studies and related analyses.</p> <p>Source: github.com/nanograv/holodeck</p> <p></p> <p> ARMI (Automated Reactor Modeling Infrastructure) is Terrapower\u2019s Python framework for simulation and analysis of nuclear reactors\u2019 core physics, including fuel performance and neutronics modeling. It builds complex reactor modeling pipelines.</p> <p>Source: github.com/terrapower/armi</p> <p></p> <p> OpenMDAO is an open-source, multidisciplinary optimization framework in Python designed for systems engineering and analysis of complex coupled systems developed by NASA Glenn Research Center. It allows linking models, setting up design variables, and running optimizations with gradient and parallel support.</p> <p>Source: github.com/OpenMDAO/OpenMDAO</p> <p></p> <p> Los Alamos National Laboratory' Bohydra is a workflow and data orchestration framework that supports design-of-experiments, optimization, UQ, and HPC workflows, particularly where multiple simulation codes must be coordinated. It\u2019s focused on ensemble studies and model-based engineering.</p> <p>Source: github.com/lanl/bohydra</p> <p></p> <p> Lawrence Livermore National Laboratory uses DOE in Zero-RK and Merlin-Spellbook to generate design-of-experiments samples for simulation-based optimization workflows. Zero-RK is a numerical ODE integrator / solver framework optimized for HPC and accurate time integration of ODE systems. It provides high-performance integrators for stiff and non-stiff problems, often used in scientific computing. Merlin-spellbook is a small utility package (often used alongside Merlin workflows) providing common building blocks for configuring workflows and tasks.</p> <p>Sources: github.com/llnl/zero-rk github.com/llnl/merlin-spellbook</p> <p></p> <p> pvOps is a Python library for photovoltaic (PV) systems data analysis, particularly for field-collected operational data, text logs, time series, IV curves, etc. It includes processing and fusion of diverse PV datasets. Sandia National Laboratories uses DOE in pvOps create samples for black box optimization.</p> <p>Source: github.com/sandialabs/pvOps</p> <p></p> <p> GEMSEO (Generic Engine for Multi-disciplinary Scenarios, Exploration and Optimization) is a general engine for multidisciplinary scenarios, exploration, and optimization \u2014 a suite for optimization, UQ, MDO, and surrogate modeling tools. It\u2019s modular and acts as an engine for exploring engineering systems under uncertainty.</p> <p>Source: github.com/gemseo/gemseo</p> <p></p> <p> The Multiobjective Optimization Group uses DOE in pyRVEA to produce DOE samples for multi-objective optimization and surrogate model workflows. It implements the Reference Vector Guided Evolutionary Algorithm for global search in design spaces.</p> <p>Source: github.com/industrial-optimization-group/pyRVEA</p> <p></p> <p> Gators is a package developed by Paypal to handle model building with big data and fast real-time pre-processing, even for a large number of QPS, using only Python. It uses DOE to generate structured design samples for model testing and performance tuning. It handles large-scale model building and real-time preprocessing on high-QPS data pipelines, providing a systematic way to explore input spaces efficiently before training or benchmarking.</p> <p>Source: github.com/paypal/gators</p> <p></p> <p> SURGroup uses DOE in UQpy (\u201cUncertainty Quantification with Python\u201d) to create design-of-experiments samples for uncertainty quantification and optimization workflows. UQpy is a general Python toolbox for uncertainty quantification in engineering and scientific contexts. It implements sampling, propagation, surrogate models, and sensitivity analysis.</p> <p>Source: github.com/SURGroup/UQpy</p>"},{"location":"reference/doe_optimal/","title":"Optimal Experimental Designs","text":"<p>The <code>pyDOE.doe_optimal</code> module provides advanced algorithms for constructing optimal experimental designs using a variety of optimality criteria and algorithms. This is useful for maximizing the information gained from experiments while minimizing the number of runs.</p> <p>Hint</p> <p>All available optimal design tools can be accessed after a simple import statement:</p> <pre><code>&gt;&gt;&gt; from pyDOE import optimal_design, generate_candidate_set\n</code></pre>"},{"location":"reference/doe_optimal/#overview","title":"Overview","text":"<p>Optimal experimental design is based on maximizing or minimizing certain properties of the information matrix, typically denoted as M, which is defined as: $$ M = X^T X $$ where \\(X\\) is the model (design) matrix.</p>"},{"location":"reference/doe_optimal/#design-matrix-model-matrix","title":"Design Matrix (Model Matrix)","text":"<p>The design (model) matrix \\(X\\) encodes the relationship between the model parameters and the input variables. For a polynomial model of degree \\(d\\) with \\(k\\) factors: $$ \\text{Linear:}\\quad y = \\beta_0 + \\sum_{i=1}^k \\beta_i x_i $$ $$ \\text{Quadratic:}\\quad y = \\beta_0 + \\sum_{i=1}^k \\beta_i x_i + \\sum_{i=1}^k \\beta_{ii} x_i^2 + \\sum_{i&lt;j} \\beta_{ij} x_i x_j $$</p> <p>The design matrix X for n points and p parameters is constructed as: $$ X = \\begin{bmatrix} 1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \\cdots &amp; (x_1^{(1)})^2 &amp; x_1^{(1)} x_2^{(1)} &amp; \\cdots \\\\ 1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \\cdots &amp; (x_1^{(2)})^2 &amp; x_1^{(2)} x_2^{(2)} &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots &amp; \\vdots &amp; \\\\ \\end{bmatrix} $$ where each row corresponds to a candidate point and each column to a model term.</p>"},{"location":"reference/doe_optimal/#efficiency-criteria","title":"Efficiency Criteria","text":"<ul> <li> <p>D-efficiency:   $$ \\text{D-efficiency} = 100 \\times (\\det(M))^{1/p} $$   $$ M = \\frac{1}{n} X^T X $$   where p is the number of parameters.</p> </li> <li> <p>A-efficiency:   $$ \\text{A-efficiency} = 100 \\times \\frac{p}{\\operatorname{tr}(M^{-1})} $$   $$ M = \\frac{1}{n} X^T X $$   where p is the number of parameters.</p> </li> </ul>"},{"location":"reference/doe_optimal/#optimality-criteria","title":"Optimality Criteria","text":"<ul> <li> <p>D-optimality: Maximizes the determinant of the information matrix.   $$ \\text{D-optimality:}\\quad \\max \\det(M) $$</p> </li> <li> <p>A-optimality: Minimizes the average variance of the parameter estimates (trace of the inverse information matrix).   $$ \\text{A-optimality:}\\quad \\min \\operatorname{tr}(M^{-1}) $$</p> </li> <li> <p>I-optimality: Minimizes the average prediction variance over the candidate set.   $$ \\text{I-optimality:}\\quad \\min \\frac{1}{|\\mathcal{X}|} \\sum_{x \\in \\mathcal{X}} x^T M^{-1} x $$</p> </li> <li> <p>C-optimality: Minimizes the variance of a linear combination of parameters.   $$ \\text{C-optimality:}\\quad \\min c^T M^{-1} c $$</p> </li> <li> <p>E-optimality: Maximizes the smallest eigenvalue of the information matrix.   $$ \\text{E-optimality:}\\quad \\max \\lambda_{\\min}(M) $$</p> </li> <li> <p>G-optimality: Minimizes the maximum prediction variance over the design space.   $$ \\text{G-optimality:}\\quad \\min \\max_{x \\in \\mathcal{X}} x^T M^{-1} x $$</p> </li> <li> <p>V-optimality: Minimizes the average variance at specific points.   $$ \\text{V-optimality:}\\quad \\min \\frac{1}{n} \\sum_{i=1}^n x_i^T M^{-1} x_i $$</p> </li> <li> <p>S-optimality: Maximizes mutual orthogonality (various definitions, often based on maximizing the sum of squared off-diagonal elements of M).</p> </li> <li> <p>T-optimality: Model discrimination (maximizes the ability to distinguish between models).</p> </li> </ul>"},{"location":"reference/doe_optimal/#algorithms","title":"Algorithms","text":"<ul> <li>Sequential (Dykstra)</li> <li>Simple Exchange (Wynn-Mitchell)</li> <li>Fedorov</li> <li>Modified Fedorov</li> <li>DETMAX</li> </ul>"},{"location":"reference/doe_optimal/#example-usage","title":"Example Usage","text":"<p>Generate a D-optimal design for a quadratic model with 2 factors:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyDOE.doe_optimal import optimal_design, generate_candidate_set\n&gt;&gt;&gt; candidates = generate_candidate_set(n_factors=2, n_levels=5)\n&gt;&gt;&gt; design, info = optimal_design(\n...     candidates=candidates,\n...     n_points=10,\n...     degree=2,\n...     criterion=\"D\",\n...     method=\"detmax\"\n... )\n&gt;&gt;&gt; print(f\"D-efficiency: {info['D_eff']:.2f}%\")\n</code></pre> <p>After an optimal design is selected and experiments are performed, we can model our system by estimating the regression parameters using: $$ \\hat{\\beta} = (X^{T} X)^{-1} X^{T} y $$ where X is the design matrix and y is the vector of observed responses.</p>"},{"location":"reference/doe_optimal/#references","title":"References","text":"<ul> <li>Atkinson, A. C., &amp; Donev, A. N. (1992). Optimum Experimental Designs. Oxford University Press.</li> <li>Fedorov, V. V. (1972). Theory of Optimal Experiments. Academic Press.</li> <li>Pukelsheim, F. (2006). Optimal Design of Experiments. SIAM.</li> <li>NIST: https://www.itl.nist.gov/div898/handbook/pri/section5/pri521.htm</li> <li>Optimal experimental design</li> </ul>"},{"location":"reference/factorial/","title":"Factorial Designs","text":"<p>In this section, the following kinds of factorial designs will be described:</p> <ul> <li>General Full-Factorial</li> <li>2-Level Full-Factorial</li> <li>2-Level Fractional-Factorial</li> <li>Plackett-Burman</li> <li>Generalized Subset Design</li> </ul> <p>Note</p> <p>All available designs can be accessed after a simple import statement: <pre><code>&gt;&gt;&gt; from pyDOE import fullfract, ff2n, fracfact, pbdesign, gsd\n</code></pre></p>"},{"location":"reference/factorial/#general-full-factorial-fullfact","title":"General Full-Factorial (<code>fullfact</code>)","text":"<p>This kind of design offers full flexibility as to the number of discrete levels for each factor in the design. Its usage is simple:</p> <pre><code>&gt;&gt;&gt; fullfact(levels) # (1)!\n&gt;&gt;&gt; fullfact([2,3])\narray([[ 0.,  0.],\n       [ 1.,  0.],\n       [ 0.,  1.],\n       [ 1.,  1.],\n       [ 0.,  2.],\n       [ 1.,  2.]])\n</code></pre> <ol> <li><code>levels</code> is array of integers.</li> </ol> <p>As can be seen in the output, the design matrix has as many columns as items in the input array.</p>"},{"location":"reference/factorial/#2-level-full-factorial-ff2n","title":"2-Level Full Factorial (<code>ff2n</code>)","text":"<p>This function is a convenience wrapper to <code>fullfact</code> that forces all the factors to have two levels each, you simple tell it how many factors to create a design for.</p> <pre><code>&gt;&gt;&gt; ff2n(3)\narray([[-1., -1., -1.],\n       [ 1., -1., -1.],\n       [-1.,  1., -1.],\n       [ 1.,  1., -1.],\n       [-1., -1.,  1.],\n       [ 1., -1.,  1.],\n       [-1.,  1.,  1.],\n       [ 1.,  1.,  1.]])\n</code></pre>"},{"location":"reference/factorial/#2-level-fractional-factorial-fracfact","title":"2-Level Fractional-Factorial (<code>fracfact</code>)","text":"<p>This function requires a little more knowledge of how the confounding will be allowed (this means that some factor effects get muddled with other interaction effects, so it's harder to distinguish between them).</p> <p>Let's assume that we just can't afford (for whatever reason) the number of runs in a full-factorial design. We can systematically decide on a fraction of the full-factorial by allowing some of the factor main effects to be confounded with other factor interaction effects. This is done by defining an alias structure that defines, symbolically, these interactions. These alias structures are written like \\(C = AB\\) or \\(I = ABC\\), or \\(AB = CD\\), etc. These define how one column is related to the others.</p> <p>For example, the alias \\(C = AB\\) or \\(I = ABC\\) indicate that there are three factors (\\(A\\), \\(B\\), and \\(C\\)) and that the main effect of factor \\(C\\) is confounded with the interaction effect of the product \\(AB\\), and by extension, \\(A\\) is confounded with \\(BC\\) and \\(B\\) is confounded with \\(AC\\). A full- factorial design with these three factors results in a design matrix with 8 runs, but we will assume that we can only afford 4 of those runs. To create this fractional design, we need a matrix with three columns, one for \\(A\\), \\(B\\), and \\(C\\), only now where the levels in the \\(C\\) column is created by the product of the \\(A\\) and \\(B\\) columns.</p> <p>The input to <code>fracfact</code> is a generator string of symbolic characters (lowercase or uppercase, but not both) separated by spaces, like::</p> <pre><code>&gt;&gt;&gt; gen = \"a b ab\"\n</code></pre> <p>This design would result in a 3-column matrix, where the third column is implicitly defined as <code>\"c = ab\"</code>. This means that the factor in the third column is confounded with the interaction of the factors in the first two columns. The design ends up looking like this;</p> <pre><code>&gt;&gt;&gt; fracfact(\"a b ab\")\narray([[-1., -1.,  1.],\n       [ 1., -1., -1.],\n       [-1.,  1., -1.],\n       [ 1.,  1.,  1.]])\n</code></pre> <p>Fractional factorial designs are usually specified using the notation \\(2^{(k-p)}\\), where \\(k\\) is the number of columns and \\(p\\) is the number of effects that are confounded. In terms of resolution level, higher is \"better\". The above design would be considered a \\(2^{(3-1)}\\) fractional factorial design, a 1/2-fraction design, or a Resolution III design (since the smallest alias \\(I=ABC\\) has three terms on the right-hand side). Another common design is a Resolution III, \\(2^{(7-4)}\\) fractional factorial and would be created using the following string generator.</p> <pre><code>&gt;&gt;&gt; fracfact(\"a b ab c ac bc abc\")\narray([[-1., -1.,  1., -1.,  1.,  1., -1.],\n       [ 1., -1., -1., -1., -1.,  1.,  1.],\n       [-1.,  1., -1., -1.,  1., -1.,  1.],\n       [ 1.,  1.,  1., -1., -1., -1., -1.],\n       [-1., -1.,  1.,  1., -1., -1.,  1.],\n       [ 1., -1., -1.,  1.,  1., -1., -1.],\n       [-1.,  1., -1.,  1., -1.,  1., -1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n</code></pre> <p>More sophisticated generator strings can be created using the \"<code>+</code>\" and \"<code>-</code>\" operators. The \"<code>-</code>\" operator swaps the levels of that column like this:</p> <pre><code>&gt;&gt;&gt; fracfact(\"a b -ab\")\narray([[-1., -1., -1.],\n       [ 1., -1.,  1.],\n       [-1.,  1.,  1.],\n       [ 1.,  1., -1.]])\n</code></pre> <p>In order to reduce confounding, we can utilize the <code>fold</code> function:</p> <pre><code>&gt;&gt;&gt; m = fracfact(\"a b ab\")\n&gt;&gt;&gt; fold(m)\narray([[-1., -1.,  1.],\n       [ 1., -1., -1.],\n       [-1.,  1., -1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1., -1.],\n       [-1.,  1.,  1.],\n       [ 1., -1.,  1.],\n       [-1., -1., -1.]])\n</code></pre> <p>Applying the fold to all columns in the design breaks the alias chains between every main factor and two-factor interactions. This means that we can then estimate all the main effects clear of any two-factor interactions. Typically, when all columns are folded, this \"upgrades\" the resolution of the design.</p> <p>By default, <code>fold</code> applies the level swapping to all columns, but we can fold specific columns (first column = 0), if desired, by supplying an array to the keyword <code>columns</code>:</p> <pre><code>&gt;&gt;&gt; fold(m, columns=[2])\narray([[-1., -1.,  1.],\n       [ 1., -1., -1.],\n       [-1.,  1., -1.],\n       [ 1.,  1.,  1.],\n       [-1., -1., -1.],\n       [ 1., -1.,  1.],\n       [-1.,  1.,  1.],\n       [ 1.,  1., -1.]])\n</code></pre> <p>Another way to reduce confounding it to scan several (or all) available fractional designs and pick the one that has less confounding. The function <code>fracfact_opt</code> performs just that. For a \\(2^{k-p}\\) fractional factorial the function scans all generators that create at most \\(2^{k-p}\\) experiments, and pick the one that has confounding on interactions of order as high as possible:</p> <pre><code>&gt;&gt;&gt; design, alias_map, alias_cost = fracfact_opt(6, 2)\n&gt;&gt;&gt; design\n\"a b c d bcd acd\"\n&gt;&gt;&gt; print(\"\\n\".join(alias_map))\na = bef = cdf = abcde\nb = aef = cde = abcdf\nc = adf = bde = abcef\nd = acf = bce = abdef\ne = abf = bcd = acdef\nf = abe = acd = bcdef\naf = be = cd = abcdef\nab = ef = acde = bcdf\nac = df = abde = bcef\nad = cf = abce = bdef\nae = bf = abcd = cdef\nbc = de = abdf = acef\nbd = ce = abcf = adef\nabc = ade = bdf = cef\nabd = ace = bcf = def\nabef = acdf = bcde\n</code></pre> <p>You can generate the human-readable <code>alias_map</code> of any design with the function <code>fracfact_aliasing</code>:</p> <pre><code>&gt;&gt;&gt; print(\"\\n\".join(fracfact_aliasing(fracfact(\"a b ab\"))[0]))\na = bc\nb = ac\nc = ab\nabc\n</code></pre> <p>Note</p> <p>Care should be taken to decide the appropriate alias structure for your design and the effects that folding has on it.</p>"},{"location":"reference/factorial/#2-level-fractional-factorial-specified-by-resolution-fracfact_by_res","title":"2-Level Fractional-Factorial specified by resolution (<code>fracfact_by_res</code>)","text":"<p>This function constructs a minimal design at given resolution. It does so by constructing a generator string with a minimal number of base factors and passes it to <code>fracfact</code>. This approach favors convenience over fine-grained control over which factors that are confounded.</p> <p>To construct a 6-factor, resolution III-design, <code>fractfact_by_res</code> is used like this;</p> <pre><code>&gt;&gt;&gt; fracfact_by_res(6, 3)\narray([[-1., -1., -1.,  1.,  1.,  1.],\n       [ 1., -1., -1., -1., -1.,  1.],\n       [-1.,  1., -1., -1.,  1., -1.],\n       [ 1.,  1., -1.,  1., -1., -1.],\n       [-1., -1.,  1.,  1., -1., -1.],\n       [ 1., -1.,  1., -1.,  1., -1.],\n       [-1.,  1.,  1., -1., -1.,  1.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.]])\n</code></pre>"},{"location":"reference/factorial/#plackett-burman","title":"Plackett-Burman (<code>pbdesign</code>)","text":"<p>Another way to generate fractional-factorial designs is through the use of Plackett-Burman designs. These designs are unique in that the number of trial conditions (rows) expands by multiples of four (e.g. 4, 8, 12, etc.). The max number of columns allowed before a design increases the number of rows is always one less than the next higher multiple of four.</p> <p>For example, I can use up to 3 factors in a design with 4 rows:</p> <pre><code>&gt;&gt;&gt; pbdesign(3)\narray([[-1., -1.,  1.],\n       [ 1., -1., -1.],\n       [-1.,  1., -1.],\n       [ 1.,  1.,  1.]])\n</code></pre> <p>But if I want to do 4 factors, the design needs to increase the number of rows up to the next multiple of four (8 in this case):</p> <pre><code>&gt;&gt;&gt; pbdesign(4)\narray([[-1., -1.,  1., -1.],\n       [ 1., -1., -1., -1.],\n       [-1.,  1., -1., -1.],\n       [ 1.,  1.,  1., -1.],\n       [-1., -1.,  1.,  1.],\n       [ 1., -1., -1.,  1.],\n       [-1.,  1., -1.,  1.],\n       [ 1.,  1.,  1.,  1.]])\n</code></pre> <p>Thus, an 8-run Plackett-Burman design can handle up to (8 - 1) = 7 factors.</p> <p>As a side note, It just so happens that the Plackett-Burman and \\(2^{(7-4)}\\) fractional factorial design are identical:</p> <pre><code>&gt;&gt;&gt; np.all(pbdesign(7)==fracfact(\"a b ab c ac bc abc\"))\nTrue\n</code></pre>"},{"location":"reference/factorial/#generalized-subset-design","title":"Generalized Subset Design (<code>gsd</code>)","text":"<p>GSD is a generalization of traditional fractional factorial designs to problems where factors can have more than two levels.</p> <p>In many application problems, factors can have categorical or quantitative factors on more than two levels. Previous reduced designs have not been able to deal with such types of problems. Full multi-level factorial designs can handle such problems but are however not economical regarding the number of experiments.</p> <p>The GSD provide balanced designs in multi-level experiments with the number of experiments reduced by a user-specified reduction factor. Complementary reduced designs are also provided analogous to fold-over in traditional fractional factorial designs.</p> <p>An example with three factors using three, four and six levels respectively reduced with a factor 4:</p> <pre><code>&gt;&gt;&gt; gsd([3, 4, 6], 4)\narray([[0, 0, 0],\n       [0, 0, 4],\n       [0, 1, 1],\n       [0, 1, 5],\n       [0, 2, 2],\n       [0, 3, 3],\n       [1, 0, 1],\n       [1, 0, 5],\n       [1, 1, 2],\n       [1, 2, 3],\n       [1, 3, 0],\n       [1, 3, 4],\n       [2, 0, 2],\n       [2, 1, 3],\n       [2, 2, 0],\n       [2, 2, 4],\n       [2, 3, 1],\n       [2, 3, 5]])\n</code></pre>"},{"location":"reference/factorial/#more-information","title":"More Information","text":"<p>If the user needs more information about appropriate designs, please consult the following articles on Wikipedia:</p> <ul> <li><code>Factorial designs</code></li> <li><code>Plackett-Burman designs</code></li> </ul> <p>There is also a wealth of information on the <code>NIST</code> website about the various design matrices that can be created as well as detailed information about designing/setting-up/running experiments in general.</p>"},{"location":"reference/low_discrepancy_sequences/","title":"Low-Discrepancy Sequences","text":"<p>Low-discrepancy sequences, also known as quasi-random sequences, are used for sampling points in a multi-dimensional space in a way that fills the space more uniformly than uncorrelated random points. These sequences are particularly valuable in computer experiments, numerical integration, global optimization, and design of experiments.</p> <p>This section includes the following quasi-random designs:</p> <ul> <li>Sukharev Grid</li> <li>Sobol' Sequence</li> <li>Halton Sequence</li> <li>Rank-1 Lattice Design</li> <li>Korobov Sequence</li> <li>Cranley-Patterson Randomization</li> </ul> <p>Hint</p> <p>All sequence functions are available with:</p> <pre><code>&gt;&gt;&gt; from pyDOE import (sukharev_grid, sobol_sequence,\n...     halton_sequence, rank1_lattice, korobov_sequence,\n...     cranley_patterson_shift)\n</code></pre>"},{"location":"reference/low_discrepancy_sequences/#sukharev_grid","title":"Sukharev Grid (<code>sukharev_grid</code>)","text":"<p>The Sukharev grid is a deterministic low-discrepancy design that places points at the centers of equally sized subcells in the unit hypercube. Unlike random sampling, no points are located on the boundaries, which minimizes the covering radius with respect to the max-norm.</p> <p>Syntax:</p> <pre><code>sukharev_grid(num_points, dimension)\n</code></pre> <ul> <li><code>num_points</code>: total number of points to generate. Must be an integer power of <code>dimension</code>.</li> <li><code>dimension</code>: dimensionality of the space.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; sukharev_grid(4, 2)\narray([[0.25, 0.25],\n       [0.25, 0.75],\n       [0.75, 0.25],\n       [0.75, 0.75]])\n</code></pre> <p>Note</p> <p>The Sukharev grid is especially useful when deterministic space-filling coverage of the design space is desired.</p>"},{"location":"reference/low_discrepancy_sequences/#see-also","title":"See Also","text":"<ul> <li>Low-discrepancy sequences</li> <li>Quasi-Monte Carlo methods</li> </ul>"},{"location":"reference/low_discrepancy_sequences/#sobol_sequence","title":"Sobol' Sequence (<code>sobol_sequence</code>)","text":"<p>Sobol' sequences are highly uniform low-discrepancy sequences commonly used in numerical methods and uncertainty quantification.</p> <p>Syntax:</p> <pre><code>sobol_sequence(num_points, dimension, scramble=False, bounds=None, seed=None)\n</code></pre> <ul> <li><code>num_points</code>: number of points to generate.</li> <li><code>dimension</code>: number of dimensions.</li> <li><code>scramble</code>: whether to use Owen scrambling (default: False).</li> <li><code>bounds</code>: optional (lower, upper) bounds for each dimension.</li> <li><code>seed</code>: optional integer seed for reproducibility.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; sobol_sequence(4, 2)\narray([[0.    , 0.    ],\n       [0.5   , 0.5   ],\n       [0.75  , 0.25  ],\n       [0.25  , 0.75  ]])\n</code></pre>"},{"location":"reference/low_discrepancy_sequences/#halton_sequence","title":"Halton Sequence (<code>halton_sequence</code>)","text":"<p>The Halton sequence generates low-discrepancy samples using mutually prime number bases for each dimension.</p> <p>Syntax:</p> <pre><code>&gt;&gt;&gt; halton_sequence(num_points, dimension, primes=None)\n</code></pre> <ul> <li><code>num_points</code>: number of samples.</li> <li><code>dimension</code>: number of dimensions.</li> <li><code>primes</code>: optional list of prime bases; defaults to the first <code>dimension</code> primes.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; halton_sequence(5, 2)\narray([[0.        , 0.        ],\n       [0.5       , 0.33333333],\n       [0.25      , 0.66666667],\n       [0.75      , 0.11111111],\n       [0.125     , 0.44444444]])\n</code></pre>"},{"location":"reference/low_discrepancy_sequences/#rank1_lattice","title":"Rank-1 Lattice Design (<code>rank1_lattice</code>)","text":"<p>A Rank-1 Lattice is a deterministic method to construct points that fill the space uniformly using modular arithmetic.</p> <p>Syntax:</p> <pre><code>rank1_lattice(num_points, dimension, generator=None)\n</code></pre> <ul> <li><code>num_points</code>: number of points.</li> <li><code>dimension</code>: dimensionality of space.</li> <li><code>generator</code>: optional list of length <code>dimension</code> used as a multiplier.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; rank1_lattice(5, 2)\narray([[0, 0],\n       [2, 2],\n       [4, 4],\n       [1, 1],\n       [3, 3]])\n</code></pre>"},{"location":"reference/low_discrepancy_sequences/#korobov_sequence","title":"Korobov Sequence (<code>korobov_sequence</code>)","text":"<p>The Korobov sequence is a special case of rank-1 lattices using a single integer base to construct all dimensions.</p> <p>Syntax:</p> <pre><code>&gt;&gt;&gt; korobov_sequence(num_points, dimension, a=None)\n</code></pre> <ul> <li><code>num_points</code>: number of points.</li> <li><code>dimension</code>: number of dimensions.</li> <li><code>generator_param</code>: optional generator integer (default: None).</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; korobov_sequence(5, 3, generator_param=3)\narray([[0, 0, 0],\n       [1, 3, 4],\n       [2, 1, 3],\n       [3, 4, 2],\n       [4, 2, 1]])\n</code></pre>"},{"location":"reference/low_discrepancy_sequences/#cranley_patterson","title":"Cranley-Patterson Randomization (<code>cranley_patterson_shift</code>)","text":"<p>The Cranley-Patterson method applies a random shift to a quasi-random sequence and wraps the result within the unit hypercube.</p> <p>Syntax:</p> <pre><code>&gt;&gt;&gt; cranley_patterson_shift(samples, seed=None)\n</code></pre> <ul> <li><code>samples</code>: input samples to randomize.</li> <li><code>seed</code>: optional random seed for reproducibility.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; from pyDOE import halton_sequence, cranley_patterson_shift\n&gt;&gt;&gt; x = halton_sequence(4, 2)\n&gt;&gt;&gt; cranley_patterson_shift(x, seed=42)\narray([[0.77395605, 0.43887844],\n       [0.27395605, 0.77221177],\n       [0.02395605, 0.10554511],\n       [0.52395605, 0.54998955]])\n</code></pre> <p>Note</p> <p>Cranley-Patterson randomization improves statistical independence between runs and is particularly helpful when replicating experiments or integrating results.</p>"},{"location":"reference/low_discrepancy_sequences/#see-also_1","title":"See Also","text":"<ul> <li>Sobol sequence</li> <li>Halton sequence</li> <li>Low-discrepancy sequences</li> </ul>"},{"location":"reference/low_discrepancy_sequences/#references","title":"References","text":"<ul> <li>Sukharev, A. G. (1971). \"Optimal strategies of the search for an extremum.\" USSR Computational Mathematics and Mathematical Physics, 11(4), 119-137.</li> <li>Cranley, R., and Patterson, T. N. L. (1976). \"Randomization of Number Theoretic Methods for Multiple Integration.\" SIAM Journal on Numerical Analysis, 13(6), 904-914.</li> <li>Halton, J. H. (1964). \"Algorithm 247: Radical-inverse quasi-random point sequence.\" Communications of the ACM, 7(12), 701.</li> <li>Sobol', I. M. (1967). \"Distribution of points in a cube and approximate evaluation of integrals.\" Zh. Vych. Mat. Mat. Fiz., 7: 784-802 (in Russian); U.S.S.R. Comput. Maths. Math. Phys., 7: 86-112.</li> </ul>"},{"location":"reference/randomized/","title":"Randomized Designs","text":"<p>In this section, the following kinds of randomized designs will be described:</p> <ul> <li>Latin-Hypercube</li> <li>Random K-Means</li> <li>Random Uniform</li> </ul> <p>Hint</p> <p>All available designs can be accessed after a simple import statement:</p> <pre><code>&gt;&gt;&gt; from pyDOE import lhs, random_k_means, random_uniform\n</code></pre>"},{"location":"reference/randomized/#latin-hypercube","title":"Latin-Hypercube (<code>lhs</code>)","text":"<p>Latin-hypercube designs can be created using the following simple syntax:</p> <pre><code>lhs(n, [samples, criterion, iterations])\n</code></pre> <p>where</p> <ul> <li>n: an integer that designates the number of factors (required)</li> <li>samples: an integer that designates the number of sample points to   generate for each factor (default: n)</li> <li> <p>criterion: a string that tells <code>lhs</code> how to sample the points   (default: None, which simply randomizes the points within the intervals):</p> </li> <li> <p>\"center\" or \"c\": center the points within the sampling intervals</p> </li> <li>\"maximin\" or \"m\": maximize the minimum distance between points, but     place the point in a randomized location within its interval</li> <li>\"centermaximin\" or \"cm\": same as \"maximin\", but centered within the     intervals</li> <li>\"correlation\" or \"corr\": minimize the maximum correlation coefficient</li> <li>\"lhsmu\" : Latin hypercube with multifimensional Uniformity. Correlation between      variable can be enforced by setting a valid correlation matrix. Description of the      algorithm can be found in Latin hypercube sampling with multidimensional uniformity.</li> </ul> <p>The output design scales all the variable ranges from zero to one which can then be transformed as the user wishes (like to a specific statistical distribution using the <code>scipy.stats.distributions</code> <code>ppf</code> (inverse cumulative distribution) function. An example of this is shown below.</p> <p>For example, if I wanted to transform the uniform distribution of 8 samples to a normal distribution (mean=0, standard deviation=1), I would do something like:</p> <pre><code>&gt;&gt;&gt; from scipy.stats.distributions import norm\n&gt;&gt;&gt; lhd = lhs(2, samples=5)\n&gt;&gt;&gt; lhd = norm(loc=0, scale=1).ppf(lhd)  # (1)!\n</code></pre> <ol> <li>this applies to both factors here</li> </ol> <p>Graphically, each transformation would look like the following, going from the blue sampled points (from using <code>lhs</code>) to the green sampled points that are normally distributed:</p> <p></p>"},{"location":"reference/randomized/#examples","title":"Examples","text":"<p>A basic 4-factor latin-hypercube design:</p> <pre><code>&gt;&gt;&gt; lhs(4, criterion='center')\narray([[ 0.875,  0.625,  0.875,  0.125],\n       [ 0.375,  0.125,  0.375,  0.375],\n       [ 0.625,  0.375,  0.125,  0.625],\n       [ 0.125,  0.875,  0.625,  0.875]])\n</code></pre> <p>Let's say we want more samples, like 10:</p> <pre><code>&gt;&gt;&gt; lhs(4, samples=10, criterion='center')\narray([[ 0.05,  0.05,  0.15,  0.15],\n       [ 0.55,  0.85,  0.95,  0.75],\n       [ 0.25,  0.25,  0.45,  0.25],\n       [ 0.45,  0.35,  0.75,  0.45],\n       [ 0.75,  0.55,  0.25,  0.55],\n       [ 0.95,  0.45,  0.35,  0.05],\n       [ 0.35,  0.95,  0.05,  0.65],\n       [ 0.15,  0.65,  0.55,  0.35],\n       [ 0.85,  0.75,  0.85,  0.85],\n       [ 0.65,  0.15,  0.65,  0.95]])\n</code></pre>"},{"location":"reference/randomized/#statistical_distribution_usage","title":"Customizing with Statistical Distributions","text":"<p>Now, let's say we want to transform these designs to be normally distributed with means = [1, 2, 3, 4] and standard deviations = [0.1, 0.5, 1, 0.25]:</p> <pre><code>&gt;&gt;&gt; design = lhs(4, samples=10)\n&gt;&gt;&gt; from scipy.stats.distributions import norm\n&gt;&gt;&gt; means = [1, 2, 3, 4]\n&gt;&gt;&gt; stdvs = [0.1, 0.5, 1, 0.25]\n&gt;&gt;&gt; for i in xrange(4):\n...     design[:, i] = norm(loc=means[i], scale=stdvs[i]).ppf(design[:, i])\n...\n&gt;&gt;&gt; design\narray([[ 0.84947986,  2.16716215,  2.81669487,  3.96369414],\n       [ 1.15820413,  1.62692745,  2.28145071,  4.25062028],\n       [ 0.99159933,  2.6444164 ,  2.14908071,  3.45706066],\n       [ 1.02627463,  1.8568382 ,  3.8172492 ,  4.16756309],\n       [ 1.07459909,  2.30561153,  4.09567327,  4.3881782 ],\n       [ 0.896079  ,  2.0233295 ,  1.54235909,  3.81888286],\n       [ 1.00415   ,  2.4246118 ,  3.3500082 ,  4.07788558],\n       [ 0.91999246,  1.50179698,  2.70669743,  3.7826346 ],\n       [ 0.97030478,  1.99322045,  3.178122  ,  4.04955409],\n       [ 1.12124679,  1.22454846,  4.52414072,  3.8707982 ]])\n</code></pre> <p>Note</p> <p>Methods for \"space-filling\" designs and \"orthogonal\" designs are in the works, so stay tuned! However, simply increasing the samples reduces the need for these anyway.</p>"},{"location":"reference/randomized/#random-k-means","title":"Random K-Means (<code>random_k_means</code>)","text":"<p>Random K-Means generates cluster centers using MacQueen's K-Means algorithm. This method creates well-distributed points in the unit hypercube by iteratively updating cluster centers based on randomly sampled points.</p> <p>Random K-Means designs can be created using the following syntax:</p> <pre><code>&gt;&gt;&gt; random_k_means(num_points,\n                   dimension,\n                   [num_steps, initial_points, callback, seed])\n</code></pre> <p>where</p> <ul> <li><code>num_points</code>: an integer that designates the number of cluster centers to generate (required)</li> <li><code>dimension</code>: an integer that designates the dimensionality of the space (required)</li> <li><code>num_steps</code>: an integer that designates the number of iterations (default: 100 * num_points)</li> <li><code>initial_points</code>: an array of initial cluster centers (default: None, which uses random points)</li> <li><code>callback</code>: a callable function called at each step with current cluster centers (default: None)</li> <li><code>seed</code>: an integer or <code>np.random.Generator</code> for reproducibility (default: None)</li> <li><code>random_state</code>: (Deprecated) Use <code>seed</code> parameter instead</li> </ul> <p>The output design contains cluster centers that are well-distributed across the unit hypercube \\([0, 1]^\\text{dimension}\\).</p>"},{"location":"reference/randomized/#examples_1","title":"Examples","text":"<p>A basic 3-point, 2-dimensional Random K-Means design:</p> <pre><code>&gt;&gt;&gt; random_k_means(3, 2, random_state=42)\narray([[0.50047407, 0.49860013],\n       [0.50168345, 0.50033893],\n       [0.49956536, 0.50004765]])\n</code></pre> <p>With custom initial points:</p> <pre><code>&gt;&gt;&gt; initial = [[0.1, 0.1], [0.5, 0.5], [0.9, 0.9]]\n&gt;&gt;&gt; random_k_means(3, 2, initial_points=initial, num_steps=50, random_state=42)\narray([[0.24854237, 0.25041155],\n       [0.50043582, 0.50058412],\n       [0.75123745, 0.74896743]])\n</code></pre>"},{"location":"reference/randomized/#random-uniform","title":"Random Uniform (<code>random_uniform</code>)","text":"<p>Random Uniform generates random samples from a uniform distribution over the half-open interval [0, 1). This is a simple wrapper around <code>numpy.random.rand</code> that provides a consistent interface with other pyDOE functions.</p> <p>Random Uniform designs can be created using the following syntax:</p> <pre><code>&gt;&gt;&gt; random_uniform(num_points, dimension)\n</code></pre> <p>where</p> <ul> <li>num_points: an integer that designates the number of random points to generate (required)</li> <li>dimension: an integer that designates the dimensionality of each point (required)</li> </ul> <p>The output design contains completely random points uniformly distributed in the unit hypercube \\([0, 1)^\\text{dimension}\\).</p>"},{"location":"reference/randomized/#examples_2","title":"Examples","text":"<p>A basic 5-point, 3-dimensional Random Uniform design:</p> <pre><code>&gt;&gt;&gt; np.random.seed(42)  # For reproducibility\n&gt;&gt;&gt; random_uniform(5, 3)\narray([[0.37454012, 0.95071431, 0.73199394],\n       [0.59865848, 0.15601864, 0.15599452],\n       [0.05808361, 0.86617615, 0.60111501],\n       [0.70807258, 0.02058449, 0.96990985],\n       [0.83244264, 0.21233911, 0.18182497]])\n</code></pre> <p>For 2D visualization:</p> <pre><code>&gt;&gt;&gt; np.random.seed(123)\n&gt;&gt;&gt; points = random_uniform(20, 2) # (1)!\n</code></pre> <ol> <li>Points are completely random with no structure</li> </ol>"},{"location":"reference/randomized/#more-information","title":"More Information","text":"<p>If the user needs more information about appropriate designs, please consult the following articles on Wikipedia:</p> <ul> <li>Latin-Hypercube designs</li> </ul> <p>There is also a wealth of information on the NIST website about the various design matrices that can be created as well as detailed information about designing/setting-up/running experiments in general.</p>"},{"location":"reference/response_surface/","title":"Response Surface Designs","text":"<p>In this section, the following kinds of response surface designs will  be described:</p> <ul> <li>Box-Behnken</li> <li>Central Composite</li> <li>Doehlert Design</li> </ul> <p>Hint</p> <p>All available designs can be accessed after a simple import statement: <pre><code>&gt;&gt;&gt; from pyDOE import bbdesign, ccdesign, doehlert_shell_design, doehlert_simplex_design\n</code></pre></p>"},{"location":"reference/response_surface/#box_behnken","title":"Box-Behnken (<code>bbdesign</code>)","text":"<p>Box-Behnken designs can be created using the following simple syntax:</p> <pre><code>bbdesign(n, center)\n</code></pre> <p>where <code>n</code> is the number of factors (at least 3 required) and <code>center</code> is the number of center points to include. If no inputs given to <code>center</code>, then a pre-determined number of points are automatically included.</p>"},{"location":"reference/response_surface/#examples","title":"Examples","text":"<p>The default 3-factor Box-Behnken design:</p> <pre><code>&gt;&gt;&gt; bbdesign(3)\narray([[-1., -1.,  0.],\n       [ 1., -1.,  0.],\n       [-1.,  1.,  0.],\n       [ 1.,  1.,  0.],\n       [-1.,  0., -1.],\n       [ 1.,  0., -1.],\n       [-1.,  0.,  1.],\n       [ 1.,  0.,  1.],\n       [ 0., -1., -1.],\n       [ 0.,  1., -1.],\n       [ 0., -1.,  1.],\n       [ 0.,  1.,  1.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.],\n       [ 0.,  0.,  0.]])\n</code></pre> <p>A customized design with four factors, but only a single center point:</p> <pre><code>&gt;&gt;&gt; bbdesign(4, center=1)\narray([[-1., -1.,  0.,  0.],\n       [ 1., -1.,  0.,  0.],\n       [-1.,  1.,  0.,  0.],\n       [ 1.,  1.,  0.,  0.],\n       [-1.,  0., -1.,  0.],\n       [ 1.,  0., -1.,  0.],\n       [-1.,  0.,  1.,  0.],\n       [ 1.,  0.,  1.,  0.],\n       [-1.,  0.,  0., -1.],\n       [ 1.,  0.,  0., -1.],\n       [-1.,  0.,  0.,  1.],\n       [ 1.,  0.,  0.,  1.],\n       [ 0., -1., -1.,  0.],\n       [ 0.,  1., -1.,  0.],\n       [ 0., -1.,  1.,  0.],\n       [ 0.,  1.,  1.,  0.],\n       [ 0., -1.,  0., -1.],\n       [ 0.,  1.,  0., -1.],\n       [ 0., -1.,  0.,  1.],\n       [ 0.,  1.,  0.,  1.],\n       [ 0.,  0., -1., -1.],\n       [ 0.,  0.,  1., -1.],\n       [ 0.,  0., -1.,  1.],\n       [ 0.,  0.,  1.,  1.],\n       [ 0.,  0.,  0.,  0.]])\n</code></pre>"},{"location":"reference/response_surface/#central_composite","title":"Central Composite (<code>ccdesign</code>)","text":"<p>Central composite designs can be created and customized using the syntax:</p> <pre><code>ccdesign(n, center, alpha, face)\n</code></pre> <p>where</p> <ul> <li><code>n</code> is the number of factors,</li> <li><code>center</code> is a 2-tuple of center points (one for the factorial block, one for the star block, default (4, 4))</li> <li><code>alpha</code> is either \"orthogonal\" (or \"o\", default) or \"rotatable\" (or \"r\")</li> <li><code>face</code> is either \"circumscribed\" (or \"ccc\", default), \"inscribed\" (or \"cci\"), or \"faced\" (or \"ccf\").</li> </ul> <p></p> <p>The two optional keyword arguments <code>alpha</code> and <code>face</code> help describe how the variance in the quadratic approximation is distributed. Please see the NIST web pages if you are uncertain which options are suitable for your situation.</p> <p>Note</p> <ul> <li>'ccc' and 'cci' can be rotatable designs, but 'ccf' cannot.</li> <li>If <code>face</code> is specified, while <code>alpha</code> is not, then the default value of <code>alpha</code> is 'orthogonal'.</li> </ul>"},{"location":"reference/response_surface/#examples_1","title":"Examples","text":"<p>Simplest input, assuming default kwargs:</p> <pre><code>&gt;&gt;&gt; ccdesign(2)\narray([[-1.        , -1.        ],\n       [ 1.        , -1.        ],\n       [-1.        ,  1.        ],\n       [ 1.        ,  1.        ],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ],\n       [-1.41421356,  0.        ],\n       [ 1.41421356,  0.        ],\n       [ 0.        , -1.41421356],\n       [ 0.        ,  1.41421356],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ]])\n</code></pre> <p>More customized input, say, for a set of computer experiments where there isn't variability so we only need a single center point:</p> <pre><code>&gt;&gt;&gt; ccdesign(3, center=(0, 1), alpha=\"r\", face=\"cci\")\narray([[-0.59460356, -0.59460356, -0.59460356],\n       [ 0.59460356, -0.59460356, -0.59460356],\n       [-0.59460356,  0.59460356, -0.59460356],\n       [ 0.59460356,  0.59460356, -0.59460356],\n       [-0.59460356, -0.59460356,  0.59460356],\n       [ 0.59460356, -0.59460356,  0.59460356],\n       [-0.59460356,  0.59460356,  0.59460356],\n       [ 0.59460356,  0.59460356,  0.59460356],\n       [-1.        ,  0.        ,  0.        ],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.        , -1.        ,  0.        ],\n       [ 0.        ,  1.        ,  0.        ],\n       [ 0.        ,  0.        , -1.        ],\n       [ 0.        ,  0.        ,  1.        ],\n       [ 0.        ,  0.        ,  0.        ]])\n</code></pre>"},{"location":"reference/response_surface/#doehlert_design","title":"Doehlert Design (<code>doehlert_shell_design</code>, <code>doehlert_simplex_design</code>)","text":"<p>An alternative and very useful design for second-order models is the uniform shell design proposed by Doehlert in 1970 1. Doehlert designs are especially advantageous when optimizing multiple variables, requiring fewer experiments than central composite designs, while providing efficient and uniform coverage of the experimental domain.</p> <p>The Doehlert design defines a spherical experimental domain and emphasizes uniform space filling. Although it is not orthogonal or rotatable, it is generally sufficient for practical applications.</p> <p>For two variables, the Doehlert design consists of a center point and six points forming a regular hexagon, situated on a circle. The total number of experiments is given by: $$ N = k^2 + k + C_0 $$ where</p> <ul> <li>\\(k\\) = number of factors (variables)</li> <li>\\(C_0\\) = number of center points</li> </ul> <p>Two implementations are included:</p> <ul> <li><code>doehlert_shell_design</code>: uses a shell-based spherical approach with optional center points.</li> <li><code>doehlert_simplex_design</code>: uses a simplex-based method to uniformly fill the design space.</li> </ul>"},{"location":"reference/response_surface/#examples_2","title":"Examples","text":"<p>Create a Doehlert design with 3 factors and 1 center point using the shell approach:</p> <pre><code>&gt;&gt;&gt; doehlert_shell_design(3, num_center_points=1)\narray([[ 0.       ,  0.       ,  0.        ],\n       [ 1.       ,  0.       ,  0.        ],\n       [-0.5      ,  0.8660254,  0.        ],\n       [-0.5      , -0.8660254,  0.        ],\n       [ 0.8660254,  0.5      ,  0.        ],\n       [ 0.8660254, -0.5      ,  0.        ],\n       ...                                  ])\n</code></pre> <p>Create a Doehlert design using the simplex approach for 3 factors:</p> <pre><code>&gt;&gt;&gt; doehlert_simplex_design(3)\narray([[ 0.      ,  0.       , 0.        ],\n       [ 1.      ,  0.       , 0.        ],\n       [ 0.      ,  0.8660254, 0.        ],\n       [ 0.      ,  0.5      , 0.81649658],\n       [-1.      ,  0.       , 0.        ],\n       [ 0.      , -0.8660254, 0.        ],\n       ...                                ])\n</code></pre> <p>Note</p> <p>Doehlert designs are recommended for response surface modeling when good space coverage and fewer experimental runs are desired.</p>"},{"location":"reference/response_surface/#more-information","title":"More Information","text":"<p>If the user needs more information about appropriate designs, please  consult the following articles:</p> <ul> <li>Box-Behnken designs</li> <li>Central composite designs</li> <li>Doehlert design</li> </ul> <p>There is also a wealth of information on the NIST website about the various design matrices that can be created as well as detailed information about designing/setting-up/running experiments in general.</p>"},{"location":"reference/sampling_designs/","title":"Sampling Designs","text":"<p>Sampling designs are structured experimental design methods used to efficiently explore parameter spaces and quantify relationships between input variables and model outputs. These methods are particularly valuable for sensitivity analysis, understanding model behavior, identifying influential parameters, and reducing computational burden by focusing on the most important variables.</p> <p>This section includes the following sampling design methods:</p> <ul> <li>Morris Method</li> <li>Saltelli Sampling</li> </ul> <p>Hint</p> <p>All sampling design functions are available with:</p> <pre><code>&gt;&gt;&gt; from pyDOE import morris_sampling, saltelli_sampling\n</code></pre>"},{"location":"reference/sampling_designs/#morris_method","title":"Morris Method (<code>morris_sampling</code>)","text":"<p>The Morris method, also known as the Morris screening method, is a computationally efficient global sensitivity analysis technique that estimates the importance of input variables by analyzing one-at-a-time (OAT) trajectories through a discretized parameter space. It is especially useful for identifying influential parameters in high-dimensional models with relatively low computational cost.</p> <p>The method constructs trajectories through the parameter space where each trajectory consists of \\(D+1\\) points (where \\(D\\) is the number of input variables), and consecutive points differ in exactly one coordinate. This allows for efficient estimation of sensitivity measures with fewer model evaluations compared to full-factorial or Monte Carlo approaches.</p> <p>Morris samples can be created using the following simple syntax:</p> <pre><code>&gt;&gt;&gt; morris_samples = morris_sampling(num_vars=3, N=10, num_levels=4, seed=128)\n</code></pre> <p>This creates 10 Morris trajectories for 3 variables with samples in the [0, 1] range, using a 4-level grid. The resulting sample matrix has shape \\((N \\times (D+1), D) = (30, 3)\\).</p> <p>Available keyword arguments:</p> <ul> <li><code>num_vars</code> (int): Number of input variables (dimensionality of the problem)</li> <li><code>N</code> (int): Number of trajectories to generate</li> <li><code>num_levels</code> (int): Number of levels in the grid (must be even, default=4)</li> <li><code>seed</code> (int, optional): Random seed for reproducibility</li> </ul>"},{"location":"reference/sampling_designs/#usage-examples","title":"Usage Examples","text":"<p>Basic usage with 2 variables:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyDOE import morris_sampling\n&gt;&gt;&gt; samples = morris_sampling(num_vars=2, N=5, num_levels=4, seed=128)\n&gt;&gt;&gt; samples.shape\n(15, 2)\n</code></pre> <p>The Morris method with different grid levels:</p> <pre><code>&gt;&gt;&gt; # Using 6 levels instead of 4\n&gt;&gt;&gt; samples_6 = morris_sampling(num_vars=2, N=5, num_levels=6, seed=128)\n&gt;&gt;&gt; samples_6.shape\n(15, 2)\n</code></pre> <p>High-dimensional example:</p> <pre><code>&gt;&gt;&gt; # 10-dimensional problem\n&gt;&gt;&gt; samples_10d = morris_sampling(num_vars=10, N=20, num_levels=4)\n&gt;&gt;&gt; samples_10d.shape\n(220, 10)  # 20 trajectories * 11 points per trajectory\n</code></pre>"},{"location":"reference/sampling_designs/#morris-method-theory","title":"Morris Method Theory","text":"<p>The Morris method is based on computing elementary effects for each input variable. For a model \\(f(\\mathbf{x})\\) where \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_D)\\), the elementary effect of the \\(i\\)-th variable is defined as:</p> \\[ EE_i = \\frac{f(x_1, \\ldots, x_i + \\Delta, \\ldots, x_D) - f(x_1, \\ldots, x_i, \\ldots, x_D)}{\\Delta} \\] <p>where \\(\\Delta\\) is the step size determined by the grid level.</p> <p>The Morris method estimates two sensitivity measures:</p> <ol> <li>Mean of elementary effects (\\(\\mu_i\\)): Indicates the overall influence of variable \\(i\\)</li> <li>Standard deviation of elementary effects (\\(\\sigma_i\\)): Indicates non-linear effects and interactions</li> </ol>"},{"location":"reference/sampling_designs/#advantages-of-the-morris-method","title":"Advantages of the Morris Method","text":"<ul> <li>Computationally efficient: requires only \\(N \\times (D+1)\\) model evaluations</li> <li>Suitable for high-dimensional problems</li> <li>Provides screening of important variables</li> <li>Good for preliminary sensitivity analysis</li> <li>Returns samples in [0, 1] hypercube that can be easily transformed to any bounds</li> </ul>"},{"location":"reference/sampling_designs/#saltelli_sampling","title":"Saltelli Sampling (<code>saltelli_sampling</code>)","text":"<p>Saltelli sampling is a specialized sampling scheme designed for computing Sobol' sensitivity indices, which are variance-based global sensitivity measures. This method is based on Sobol' sequences and provides an efficient way to estimate first-order, total-order, and optionally second-order Sobol' indices.</p> <p>Unlike the Morris method which provides qualitative screening, Saltelli sampling enables quantitative variance-based sensitivity analysis. It uses quasi-random low-discrepancy Sobol' sequences which provide better convergence properties compared to random sampling methods.</p> <p>Saltelli samples can be created using the following simple syntax:</p> <pre><code>&gt;&gt;&gt; saltelli_samples = saltelli_sampling(num_vars=3, N=1024,\n                                         calc_second_order=True, seed=128)\n</code></pre> <p>This creates Saltelli samples for 3 variables with 1024 base samples, including second-order interaction terms. The resulting sample matrix has shape  \\((N \\times (2D+2), D) = (8192, 3)\\). All samples are in the [0, 1] range.</p> <p>Available keyword arguments:</p> <ul> <li><code>num_vars</code> (int): Number of input variables (dimensions)</li> <li><code>N</code> (int): Base sample size (ideally a power of 2 for optimal Sobol' sequence properties)</li> <li><code>calc_second_order</code> (bool): Include second-order interaction terms (default=True)</li> <li><code>skip_values</code> (int, optional): Number of Sobol' points to skip (set automatically if None)</li> <li><code>scramble</code> (bool): Whether to use scrambling for Sobol' sequence (default=False)</li> <li><code>seed</code> (int, optional): Random seed (only used if scramble=True)</li> </ul>"},{"location":"reference/sampling_designs/#usage-examples_1","title":"Usage Examples","text":"<p>Basic usage with first and total-order indices:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyDOE import saltelli_sampling\n&gt;&gt;&gt; samples = saltelli_sampling(num_vars=3, N=1024, seed=128)\n&gt;&gt;&gt; samples.shape\n(8192, 3)  # (1)!\n</code></pre> <ol> <li>N * (2 * D + 2) = 1024 * 8</li> </ol> <p>First-order indices only (excluding second-order interactions):</p> <pre><code>&gt;&gt;&gt; samples_first = saltelli_sampling(num_vars=3, N=1024, \n...                                   calc_second_order=False, seed=128)\n&gt;&gt;&gt; samples_first.shape\n(5120, 3)  # (1)!\n</code></pre> <ol> <li>N * (D + 2) = 1024 * 5</li> </ol> <p>Using scrambled Sobol' sequences:</p> <pre><code>&gt;&gt;&gt; samples_scrambled = saltelli_sampling(num_vars=3, N=1024,\n...                                       scramble=True, seed=128)\n</code></pre> <p>High-dimensional example:</p> <pre><code>&gt;&gt;&gt; # 8-dimensional problem\n&gt;&gt;&gt; samples_8d = saltelli_sampling(num_vars=8, N=2048)\n&gt;&gt;&gt; samples_8d.shape\n(36864, 8)  # (1)!\n</code></pre> <ol> <li>2048 * (2*8 + 2) = 2048 * 18</li> </ol>"},{"location":"reference/sampling_designs/#saltelli-sampling-theory","title":"Saltelli Sampling Theory","text":"<p>The Saltelli sampling scheme is designed to efficiently compute Sobol' indices, which decompose the total variance of a model output into contributions from individual variables and their interactions: $$ \\text{Var}(Y) = \\sum_{i} V_i + \\sum_{i&lt;j} V_{ij} + \\ldots + V_{1,2,\\ldots,D} $$ where \\(V_i\\) represents the first-order effect of variable \\(i\\), and \\(V_{ij}\\) represents the second-order interaction between variables \\(i\\) and \\(j\\).</p>"},{"location":"reference/sampling_designs/#sobol-indices","title":"Sobol' Indices","text":"<ol> <li>First-order index: \\(S_i = \\frac{V_i}{\\text{Var}(Y)}\\)</li> <li>Total-order index: \\(S_{T_i} = \\frac{E_{\\mathbf{x}_{\\sim i}}[\\text{Var}_{x_i}(Y|\\mathbf{x}_{\\sim i})]}{\\text{Var}(Y)}\\)</li> <li>Second-order index: \\(S_{ij} = \\frac{V_{ij}}{\\text{Var}(Y)}\\)</li> </ol>"},{"location":"reference/sampling_designs/#sample-matrix-structure","title":"Sample Matrix Structure","text":"<p>For \\(D\\) variables and \\(N\\) base samples, the Saltelli method generates:</p> <ul> <li>With second-order terms: \\(N \\times (2D + 2)\\) samples</li> <li>First-order only: \\(N \\times (D + 2)\\) samples</li> </ul> <p>The matrix consists of:</p> <ol> <li>\\(N\\) samples from matrix A</li> <li>\\(N \\times D\\) samples from matrix A with individual columns replaced by B</li> <li>(Optional) \\(N \\times D\\) samples from matrix B with individual columns replaced by A</li> <li>\\(N\\) samples from matrix B</li> </ol>"},{"location":"reference/sampling_designs/#advantages-of-saltelli-sampling","title":"Advantages of Saltelli Sampling","text":"<ul> <li>Provides quantitative sensitivity measures (Sobol' indices)</li> <li>Uses low-discrepancy sequences for better convergence</li> <li>Enables computation of interaction effects</li> <li>Well-established theoretical foundation</li> <li>Suitable for variance-based global sensitivity analysis</li> </ul>"},{"location":"reference/sampling_designs/#computational-requirements","title":"Computational Requirements","text":"<ul> <li>First-order + Total-order: \\(N \\times (D + 2)\\) model evaluations</li> <li>Including second-order: \\(N \\times (2D + 2)\\) model evaluations</li> </ul>"},{"location":"reference/sampling_designs/#important-notes","title":"Important Notes","text":"<ul> <li>\\(N\\) should ideally be a power of 2 for optimal Sobol' sequence properties</li> <li>Larger \\(N\\) values provide more accurate sensitivity estimates</li> <li>The method requires more model evaluations than Morris but provides quantitative results</li> <li>Suitable for models where variance-based sensitivity analysis is needed</li> <li>The implementation automatically skips initial Sobol' sequence points to improve quality</li> <li>All samples are returned in the [0,1] hypercube and should be transformed to desired bounds</li> </ul>"},{"location":"reference/sampling_designs/#references","title":"References","text":"<ul> <li>Campolongo, F., Cariboni, J., &amp; Saltelli, A. (2007). An effective screening design for sensitivity analysis of large models. Environmental Modelling &amp; Software, 22(10), 1509-1518.</li> <li>Campolongo, F., Saltelli, A., &amp; Cariboni, J. (2011). From screening to quantitative sensitivity analysis. A unified approach. Computer Physics Communications, 182, 978-988.</li> <li>Morris, M. D. (1991). Factorial sampling plans for preliminary computational experiments. Technometrics, 33(2), 161-174.</li> <li>Saltelli, A. (2002). Making best use of model evaluations to compute sensitivity indices. Computer Physics Communications, 145(2), 280-297.</li> <li>Sobol', I. M. (2001). Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates. Mathematics and Computers in Simulation, 55(1-3), 271-280.</li> </ul>"},{"location":"reference/sparse_grid/","title":"Sparse Grid Designs","text":"<p>The <code>pyDOE</code> module provides sparse grid construction using Smolyak's construction (Smolyak, 1963)  for generating experimental designs with hierarchical grid structures that maintain good  space-filling properties while requiring significantly fewer points than traditional  full grid approaches in high-dimensional spaces.</p> <p>Hint</p> <p>All sparse grid functions are available with:</p> <pre><code>&gt;&gt;&gt; from pyDOE import doe_sparse_grid, sparse_grid_dimension\n</code></pre>"},{"location":"reference/sparse_grid/#overview","title":"Overview","text":"<p>This implementation is based on the MATLAB Sparse Grid Interpolation Toolbox by Andreas Klimke (Klimke &amp; Wohlmuth, 2005) and provides exact compatibility with MATLAB spinterp's spdim function.</p> <p>Sparse grids use Smolyak's construction to overcome the curse of dimensionality: while a full grid in :math:<code>d</code> dimensions with :math:<code>n</code> points per dimension requires  :math:<code>n^d</code> total points, sparse grids require significantly fewer points.</p> <p>Grid Types:     - Clenshaw-Curtis: Nested grids based on Chebyshev polynomials (recommended)     - Chebyshev: Points at Chebyshev polynomial extrema     - Gauss-Patterson: Quadrature-based points</p>"},{"location":"reference/sparse_grid/#sparse-grid-design-doe_sparse_grid","title":"Sparse Grid Design (<code>doe_sparse_grid</code>)","text":"<p>The main function for generating sparse grid designs using Smolyak's construction. This implementation exactly matches MATLAB spinterp's theoretical point counts.</p> <p>Syntax:</p> <pre><code>&gt;&gt;&gt; doe_sparse_grid(n_level, n_factors, grid_type='clenshaw_curtis')\n</code></pre> <p>Parameters:</p> <ul> <li> <p><code>n_level</code> : int     Sparse grid level. Higher levels provide more points and better accuracy.     Level 0 gives a single center point, higher levels add structured points.</p> </li> <li> <p><code>n_factors</code> : int     Number of factors/dimensions in the design space.</p> </li> <li> <p><code>grid_type</code> : {'clenshaw_curtis', 'chebyshev', 'gauss_patterson'}, default 'clenshaw_curtis'     Type of 1D grid points to use:</p> <ul> <li><code>'clenshaw_curtis'</code>: Nested Clenshaw-Curtis points (recommended)</li> <li><code>'chebyshev'</code>: Chebyshev polynomial extrema points  </li> <li><code>'gauss_patterson'</code>: Gauss-Patterson quadrature points</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li><code>design</code> : ndarray of shape (n_points, n_factors)     Sparse grid design points in the unit hypercube [0, 1]^n_factors.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyDOE import doe_sparse_grid\n\n&gt;&gt;&gt; # Basic 2D sparse grid\n&gt;&gt;&gt; design = doe_sparse_grid(n_level=3, n_factors=2)\n&gt;&gt;&gt; print(f\"Generated {len(design)} points in 2D\")\nGenerated 29 points in 2D\n\n&gt;&gt;&gt; # High-dimensional sparse grid\n&gt;&gt;&gt; design = doe_sparse_grid(n_level=4, n_factors=4)\n&gt;&gt;&gt; print(f\"4D design with {len(design)} points\")\n4D design with 177 points\n\n&gt;&gt;&gt; # Chebyshev sparse grid\n&gt;&gt;&gt; design = doe_sparse_grid(n_level=2, n_factors=3, grid_type='chebyshev')\n&gt;&gt;&gt; print(f\"Chebyshev grid: {design.shape}\")\nChebyshev grid: (25, 3)\n</code></pre> <p>Note</p> <p>The point counts follow exact polynomial formulas from Schreiber (2000) that match MATLAB spinterp's spdim function:</p> <ul> <li>Level 0: 1 point (center)</li> <li>Level 1: 2*d + 1 points  </li> <li>Level 2: 2d\u00b2 + 2d + 1 points</li> <li>Higher levels: polynomial growth in dimension</li> </ul>"},{"location":"reference/sparse_grid/#sparse-grid-dimension-sparse_grid_dimension","title":"Sparse Grid Dimension (<code>sparse_grid_dimension</code>)","text":"<p>Returns the expected number of points in a sparse grid without generating the actual points. This is useful for planning and memory estimation.</p> <p>Syntax:</p> <pre><code>&gt;&gt;&gt; sparse_grid_dimension(n_level, n_factors)\n</code></pre> <ul> <li><code>n_level</code>: Sparse grid level (integer \u2265 0)</li> <li><code>n_factors</code>: Number of factors/dimensions (integer \u2265 1)</li> </ul> <p>Returns: Integer number of points that would be generated</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; # Check point count before generation\n&gt;&gt;&gt; point_count = sparse_grid_dimension(n_level=5, n_factors=8)\n&gt;&gt;&gt; print(f\"Level 5, 8D grid will have {point_count} points\")\n\n&gt;&gt;&gt; # Compare different levels\n&gt;&gt;&gt; for level in range(1, 6):\n...     count = sparse_grid_dimension(level, 4)\n...     print(f\"Level {level}: {count} points\")\n</code></pre>"},{"location":"reference/sparse_grid/#mathematical-background","title":"Mathematical Background","text":""},{"location":"reference/sparse_grid/#smolyaks-construction","title":"Smolyak's Construction","text":"<p>Sparse grids are constructed using Smolyak's formula (Smolyak, 1963), which combines univariate interpolation rules. For a multivariate function :math:<code>f</code>, the sparse grid  interpolation operator is:</p> \\[ \\mathcal{A}^d_{n} f = \\sum_{|\\mathbf{i}|_1 \\leq n+d-1} (-1)^{n+d-1-|\\mathbf{i}|_1}  \\binom{d-1}{n+d-1-|\\mathbf{i}|_1} \\bigotimes_{j=1}^d \\mathcal{U}^{i_j} \\] <p>where \\(\\mathbf{i} = (i_1, \\ldots, i_d)\\) is a multi-index and \\(|\\mathbf{i}|_1 = i_1 + \\cdots + i_d\\).</p>"},{"location":"reference/sparse_grid/#point-count-formula","title":"Point Count Formula","text":"<p>For sparse grid level \\(n\\) and dimension \\(d\\): $$ N(n,d) = \\sum_{k=0}^{n} \\binom{n-k+d-1}{d-1} \\cdot 2^k $$</p>"},{"location":"reference/sparse_grid/#grid-types","title":"Grid Types","text":"<ul> <li>Clenshaw-Curtis: Nested grids based on Chebyshev polynomials (recommended)</li> <li>Chebyshev: Points at Chebyshev polynomial extrema</li> <li>Gauss-Patterson: Quadrature-based nested points</li> </ul> <p>For detailed mathematical exposition, see the SPINTERP documentation.</p>"},{"location":"reference/sparse_grid/#example-usage","title":"Example Usage","text":"<p>Generate a sparse grid design for 3 factors at level 4:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pyDOE import doe_sparse_grid, sparse_grid_dimension\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Check point count first\n&gt;&gt;&gt; n_points = sparse_grid_dimension(n_level=4, n_factors=3)\n&gt;&gt;&gt; print(f\"Expected points: {n_points}\")\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Generate sparse grid\n&gt;&gt;&gt; design = doe_sparse_grid(n_level=4, n_factors=3)\n&gt;&gt;&gt; print(f\"Generated: {design.shape}\")\nGenerated: (177, 3)\n</code></pre>"},{"location":"reference/sparse_grid/#references","title":"References","text":"<ul> <li> <p>Genz, A. (1987). A package for testing multiple integration subroutines.    In P. Keast &amp; G. Fairweather (Eds.), Numerical Integration: Recent Developments,    Software and Applications (pp. 337-340). Reidel. ISBN: 9027725144. https://doi.org/10.1007/978-94-009-3889-2_33</p> </li> <li> <p>Klimke, A., &amp; Wohlmuth, B. (2005). Algorithm 847: SPINTERP: Piecewise    multilinear hierarchical sparse grid interpolation in MATLAB. ACM Transactions on    Mathematical Software, 31(4), 561-579. https://doi.org/10.1145/1114268.1114275</p> </li> <li> <p>Klimke, A. (2006). SPINTERP V2.1: Piecewise multilinear hierarchical    sparse grid interpolation in MATLAB: Documentation.</p> </li> <li> <p>Smolyak, S. (1963). Quadrature and interpolation formulas for tensor    products of certain classes of functions. Doklady Akademii Nauk SSSR, 4, 240-243.</p> </li> </ul> <p>Original MATLAB Documentation:</p> <ul> <li>SPINTERP Toolbox</li> <li>Mathematical Details</li> <li>Implementation Guide</li> <li>Function Reference</li> </ul>"},{"location":"reference/taguchi/","title":"Taguchi Designs","text":"<p>Inspired by Taguchi design methodology and the orthogonal arrays developed by Genichi Taguchi, this module provides utilities for generating Taguchi experimental designs and computing Signal-to-Noise Ratios (SNR), based on a library of orthogonal arrays.</p> <p>Taguchi designs allow for systematic exploration of factor effects using a minimal number of experiments, offering a robust alternative to full factorial designs.</p> <p>Sources of orthogonal arrays:</p> <ul> <li>University of York Orthogonal Arrays</li> <li>A Library of Orthogonal Arrays by N. J. A. Sloane</li> </ul>"},{"location":"reference/taguchi/#references","title":"References","text":"<ul> <li>Taguchi G., Chowdhury S., Wu Y. (2005). Taguchi's Quality Engineering Handbook. Wiley.</li> <li>Montgomery D. C. (2017). Design and Analysis of Experiments. Wiley.</li> <li>What are Taguchi designs?</li> </ul> <p>Hint</p> <p>All functions are available after importing:</p> <pre><code>&gt;&gt;&gt; from pyDOE import (list_orthogonal_arrays, get_orthogonal_array,\n...     taguchi_design, compute_snr, TaguchiObjective)\n</code></pre>"},{"location":"reference/taguchi/#available-orthogonal-arrays","title":"Available Orthogonal Arrays","text":"<p>You can list all available Taguchi orthogonal arrays:</p> <pre><code>&gt;&gt;&gt; list_orthogonal_arrays()\n['L4(2^3)', 'L8(2^7)', 'L9(3^4)', 'L12(2^11)', 'L16(2^15)', ...]\n</code></pre> <p>Each array is described using notation like \"\\(L_9(3^4)\\)\", meaning an array with 9 runs and 4 factors each at 3 levels.</p>"},{"location":"reference/taguchi/#retrieving-an-orthogonal-array","title":"Retrieving an Orthogonal Array","text":"<p>Get a numeric orthogonal array by name:</p> <pre><code>&gt;&gt;&gt; get_orthogonal_array('L4(2^3)')\narray([[0, 0, 0],\n       [0, 1, 1],\n       [1, 0, 1],\n       [1, 1, 0]])\n</code></pre> <p>The arrays use zero-indexed factor levels.</p>"},{"location":"reference/taguchi/#generating-a-taguchi-design-matrix","title":"Generating a Taguchi Design Matrix","text":"<p>Generate a concrete experimental matrix using factor levels:</p> <pre><code>&gt;&gt;&gt; levels = [\n...     [\"Low\", \"High\"], # Factor 1\n...     [\"A\"  , \"B\"   ], # Factor 2\n...     [10   , 20    ], # Factor 3\n... ]\n&gt;&gt;&gt; taguchi_design('L4(2^3)', levels)\narray([['Low', 'A', 10],\n       ['Low', 'B', 20],\n       ['High', 'A', 20],\n       ['High', 'B', 10]], dtype=object)\n</code></pre> <p>The design matrix replaces coded levels with actual settings for each factor.</p>"},{"location":"reference/taguchi/#signal-to-noise-ratio-snr","title":"Signal-to-Noise Ratio (SNR)","text":"<p>Taguchi designs often focus on improving robustness, measured using the Signal-to-Noise Ratio (SNR).</p> <p>Three objectives are supported:</p> <ul> <li><code>LARGER_IS_BETTER</code></li> <li><code>SMALLER_IS_BETTER</code></li> <li><code>NOMINAL_IS_BEST</code></li> </ul> <p>Compute SNR for repeated measurements from one trial:</p> <pre><code>&gt;&gt;&gt; responses = np.array([90, 95, 93])\n&gt;&gt;&gt; compute_snr(responses, objective=TaguchiObjective.LARGER_IS_BETTER)\n39.133...\n</code></pre> <p>SNR values are returned in decibels (dB).</p> <p>Note</p> <p><code>compute_snr</code> uses logarithmic transformations and assumes the responses are positive for <code>LARGER_IS_BETTER</code> and <code>SMALLER_IS_BETTER</code>.</p>"},{"location":"reference/taguchi/#enum-for-objective","title":"Enum for Objective","text":"<p>The optimization objective is specified using the enumeration <code>TaguchiObjective</code>:</p> <pre><code>&gt;&gt;&gt; TaguchiObjective.LARGER_IS_BETTER\n&lt;TaguchiObjective.LARGER_IS_BETTER: 'larger is better'&gt;\n</code></pre>"},{"location":"reference/taguchi/#summary","title":"Summary","text":"<p>The Taguchi design utilities allow you to:</p> <ul> <li>Retrieve orthogonal arrays by name.</li> <li>Build concrete design matrices with specified factor levels.</li> <li>Evaluate robustness using Signal-to-Noise Ratios.</li> </ul> <p>For more details, see:</p> <ul> <li>NIST Engineering Statistics Handbook - Taguchi Designs</li> </ul>"},{"location":"theory/introduction/","title":"What is Experimental Design?","text":"<p>In an experiment, we deliberately change one or more process variables (or factors) in order to observe the effect the changes have on one or more response variables. The (statistical) design of experiments (DOE) is an efficient procedure for planning experiments so that the data obtained can be analyzed to yield valid and objective conclusions.</p> <p>DOE begins with determining the objectives of an experiment and selecting the process factors for the study. An Experimental Design is the laying out of a detailed experimental plan in advance of doing the experiment. Well chosen experimental designs maximize the amount of information that can be obtained for a given amount of experimental effort.</p> <p>The statistical theory underlying DOE generally begins with the concept of process models.</p>"},{"location":"theory/introduction/#process-models-for-doe","title":"Process Models for DOE","text":"<p>It is common to begin with a process model of the black box type, with several discrete or continuous input factors that can be controlled--that is, varied at will by the experimenter--and one or more measured output responses. The output responses are assumed continuous. Experimental data are used to derive an empirical (approximation) model linking the outputs and inputs. These empirical models generally contain first and second-order terms.</p> <p>Often the experiment has to account for a number of uncontrolled factors that may be discrete, such as different machines or operators, and/or continuous such as ambient temperature or humidity. Figure 1 illustrates this situation.</p> <p> </p> <p>Figure 1:  A Black Box Process Model Schematic</p> <p>The most common empirical models fit to the experimental data take either a linear form or quadratic form.</p> <p>A linear model with two factors, \\(X_1\\) and \\(X_2\\), can be written as $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_{12} X_1 X_2 + \\text{experimental error} $$</p> <p>Here, \\(Y\\) is the response for given levels of the main effects \\(X_1\\) and \\(X_2\\) and the \\(X_1X_2\\) term is included to account for a possible interaction effect between \\(X_1\\) and \\(X_2\\). The constant \\(\\beta_0\\) is the response of \\(Y\\) when both main effects are 0.</p> <p>For a more complicated example, a linear model with three factors \\(X_1\\), \\(X_2\\), \\(X_3\\) and one response, \\(Y\\), would look like (if all possible terms were included in the model) $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_{12} X_1 X_2 + \\beta_{13} X_1 X_3 + \\beta_{23} X_2 X_3 + \\beta_{123} X_1 X_2 X_3 \\newline + \\text{ experimental error} $$</p> <p>The three terms with single \\(X's\\) are the main effects. For three factors there are \\(k(k-1)/2 = 3*2/2 = 3\\) two-way interaction terms and 1 three-way interaction term (often omitted for simplicity). When the experimental data are analyzed, all unknown \\(\\beta\\) parameters are estimated and the coefficients of the \\(X\\) terms are tested to see which ones are significantly different from 0.</p> <p>A second-order (quadratic) model (typically used in response surface DOE's when curvature is suspected) does not include the three-way interaction term but adds three squared terms to the linear model, namely $$ \\beta_{11} X_1^2 + \\beta_{22} X_2^2 + \\beta_{33} X_3^2 $$</p> <p>Note</p> <p>A full model could include many cross-product (or interaction) terms involving squared \\(X's\\). However, in general these terms are not needed and most DOE software defaults to leaving them out of the model.</p>"},{"location":"theory/introduction/doe-steps/","title":"What are the steps of DOE?","text":"<p>Obtaining good results from a DOE involves these seven steps:</p> <ol> <li>Set objectives</li> <li>Select process variables</li> <li>Select an experimental design</li> <li>Execute the design</li> <li>Check that the data are consistent with the experimental assumptions</li> <li>Analyze and interpret the results</li> <li>Use/present the results (may lead to further runs or DOE's).</li> </ol>"},{"location":"theory/introduction/doe-steps/#practical-considerations","title":"Practical Considerations","text":"<p>Important practical considerations in planning and running experiments are</p> <ul> <li>Check performance of gauges/measurement devices first.</li> <li>Keep the experiment as simple as possible.</li> <li>Check that all planned runs are feasible.</li> <li>Watch out for process drifts and shifts during the run.</li> <li>Avoid unplanned changes (e.g., swap operators at halfway point).</li> <li>Allow some time (and back-up material) for unexpected events.</li> <li>Obtain buy-in from all parties involved.</li> <li>Maintain effective ownership of each step in the experimental plan.</li> <li>Preserve all the raw data--do not keep only summary averages!</li> <li>Record everything that happens.</li> <li>Reset equipment to its original state after the experiment.</li> </ul>"},{"location":"theory/introduction/doe-steps/#the-sequential-or-iterative-approach-to-doe","title":"The Sequential or Iterative Approach to DOE","text":"<p>It is often a mistake to believe that one big experiment will give the answer.</p> <p>A more useful approach to experimental design is to recognize that while one experiment might provide a useful result, it is more common to perform two or three, or maybe more, experiments before a complete answer is attained. In other words, an iterative approach is best and, in the end, most economical. Putting all one's eggs in one basket is not advisable.</p> <p>The reason an iterative approach frequently works best is because it is logical to move through stages of experimentation, each stage providing insight as to how the next experiment should be run.</p>"},{"location":"theory/introduction/doe-uses/","title":"What are the uses of DOE?","text":"<p>Below are seven examples illustrating situations in which experimental design can be used effectively.</p>"},{"location":"theory/introduction/doe-uses/#choosing-between-alternatives-comparative-experiment","title":"Choosing Between Alternatives (Comparative Experiment)","text":"<p>Supplier A vs. supplier B? Which new additive is the most effective? Is catalyst `x' an improvement over the existing catalyst? These and countless other choices between alternatives can be presented to us in a never-ending parade. Often we have the choice made for us by outside factors over which we have no control. But in many cases we are also asked to make the choice. It helps if one has valid data to back up one's decision.</p> <p>The preferred solution is to agree on a measurement by which competing choices can be compared, generate a sample of data from each alternative, and compare average results. The best average outcome will be our preference. We have performed a comparative experiment!</p> <p>Sometimes this comparison is performed under one common set of conditions. This is a comparative study with a narrow scope - which is suitable for some initial comparisons of possible alternatives. Other comparison studies, intended to validate that one alternative is preferred over a wide range of conditions, will purposely and systematically vary the background conditions under which the primary comparison is made in order to reach a conclusion that will be proven valid over a broad scope. We discuss experimental designs for each of these types of comparisons in Sections 5.3.3.1 and 5.3.3.2.</p>"},{"location":"theory/introduction/doe-uses/#selecting-the-key-factors-affecting-a-response-screening-experiments","title":"Selecting the Key Factors Affecting a Response (Screening Experiments)","text":"<p>Often there are many possible factors, some of which may be critical and others which may have little or no effect on a response. It may be desirable, as a goal by itself, to reduce the number of factors to a relatively small set (2-5) so that attention can be focussed on controlling those factors with appropriate specifications, control charts, etc.</p> <p>Screening experiments are an efficient way, with a minimal number of runs, of determining the important factors. They may also be used as a first step when the ultimate goal is to model a response with a response surface. We will discuss experimental designs for screening a large number of factors in Sections 5.3.3.3, 5.3.3.4 and 5.3.3.5.</p>"},{"location":"theory/introduction/doe-uses/#response-surface-modeling-a-process","title":"Response Surface Modeling a Process","text":"<p>Once one knows the primary variables (factors) that affect the responses of interest, a number of additional objectives may be pursued. These include:</p> <ul> <li>Hitting a Target</li> <li>Maximizing or Minimizing a Response</li> <li>Reducing Variation</li> <li>Making a Process Robust</li> <li>Seeking Multiple Goals</li> </ul> <p>What each of these purposes have in common is that experimentation is used to fit a model that may permit a rough, local approximation to the actual surface. Given that the particular objective can be met with such an approximate model, the experimental effort is kept to a minimum while still achieving the immediate goal.</p> <p>These response surface modeling objectives will now be briefly expanded upon.</p>"},{"location":"theory/introduction/doe-uses/#hitting-a-target","title":"Hitting a Target","text":"<p>This is a frequently encountered goal for an experiment.</p> <p>One might try out different settings until the desired target is hit consistently. For example, a machine tool that has been recently overhauled may require some setup tweaking before it runs on target. Such action is a small and common form of experimentation. However, rather than experimenting in an ad hoc manner until we happen to find a setup that hits the target, one can fit a model estimated from a small experiment and use this model to determine the necessary adjustments to hit the target.</p> <p>More complex forms of experimentation, such as the determination of the correct chemical mix of a coating that will yield a desired refractive index for the dried coat (and simultaneously achieve specifications for other attributes), may involve many ingredients and be very sensitive to small changes in the percentages in the mix. Fitting suitable models, based on sequentially planned experiments, may be the only way to efficiently achieve this goal of hitting targets for multiple responses simultaneously.</p>"},{"location":"theory/introduction/doe-uses/#maximizing-or-minimizing-a-response","title":"Maximizing or Minimizing a Response","text":"<p>Many processes are being run at sub-optimal settings, some of them for years, even though each factor has been optimized individually over time. Finding settings that increase yield or decrease the amount of scrap and rework represent opportunities for substantial financial gain. Often, however, one must experiment with multiple inputs to achieve a better output. Section 5.3.3.6 on second-order designs plus material in Section 5.5.3 will be useful for these applications.</p> <p></p> <p>Figure 1 Pathway up the process response surface to an optimum</p>"},{"location":"theory/introduction/doe-uses/#reducing-variation","title":"Reducing Variation","text":"<p>A process may be performing with unacceptable consistency, meaning its internal variation is too high.</p> <p>Excessive variation can result from many causes. Sometimes it is due to the lack of having or following standard operating procedures. At other times, excessive variation is due to certain hard-to-control inputs that affect the critical output characteristics of the process. When this latter situation is the case, one may experiment with these hard-to-control factors, looking for a region where the surface is flatter and the process is easier to manage. To take advantage of such flatness in the surface, one must use designs - such as the second-order designs of Section 5.3.3.6 - that permit identification of these features. Contour or surface plots are useful for elucidating the key features of these fitted models. See also 5.5.3.1.4.</p> <p></p> <p>Figure 2 Process before variation reduced</p> <p>It might be possible to reduce the variation by altering the setpoints (recipe) of the process, so that it runs in a more stable region as expressed in following illustration.</p> <p></p> <p>Figure 3: Process after variation reduced</p> <p>Finding this new recipe could be the subject of an experiment, especially if there are many input factors that could conceivably affect the output.</p>"},{"location":"theory/introduction/doe-uses/#making-a-process-robust","title":"Making a Process Robust","text":"<p>An item designed and made under controlled conditions will be later field tested in the hands of the customer and may prove susceptible to failure modes not seen in the lab or thought of by design. An example would be the starter motor of an automobile that is required to operate under extremes of external temperature. A starter that performs under such a wide range is termed robust to temperature.</p> <p>Designing an item so that it is robust calls for a special experimental effort. It is possible to stress the item in the design lab and so determine the critical components affecting its performance. A different gauge of armature wire might be a solution to the starter motor, but so might be many other alternatives. The correct combination of factors can be found only by experimentation.</p>"},{"location":"theory/introduction/doe-uses/#seeking-multiple-goals","title":"Seeking Multiple Goals","text":"<p>A product or process seldom has just one desirable output characteristic. There are usually several, and they are often interrelated so that improving one will cause a deterioration of another. For example: rate vs. consistency; strength vs. expense; etc.</p> <p>Any product is a trade-off between these various desirable final characteristics. Understanding the boundaries of the trade-off allows one to make the correct choices. This is done by either constructing some weighted objective function (`desirability function') and optimizing it, or examining contour plots of responses generated by a computer program, as given below.</p> <p></p> <p>FIGURE 4  Overlaid contour plot of Deposition Rate and Capability (Cp)</p>"},{"location":"theory/introduction/doe-uses/#regression-modeling","title":"Regression Modeling","text":"<p>Sometimes we require more than a rough approximating model over a local region. In such cases, the standard designs presented in this chapter for estimating first- or second-order polynomial models may not suffice. Chapter 4 covers the topic of experimental design and analysis for fitting general models for a single explanatory factor. If one has multiple factors, and either a nonlinear model or some other special model, the computer-aided designs of Section 5.5.2 may be useful.</p>"}]}